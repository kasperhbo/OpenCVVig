"""SAMPLE OF HOW TO IMPLEMENT IN DJANGO:in de view denk ik? weet niet wat het beste is1.    def video_feed(request):        def generate():            while True:                # Capture your stitched frame from OpenCV                _, jpeg = cv2.imencode('.jpg', final)                yield (b'--frame\r\n'                       b'Content-Type: image/jpeg\r\n\r\n' + jpeg.tobytes() + b'\r\n\r\n')        return StreamingHttpResponse(generate(), content_type='multipart/x-mixed-replace; boundary=frame')2.    voeg toe aan je url pattern3.   <img src="{% url 'video_feed' %}" />TODO:    1.  video's achter elkaar uitlezen    2.  video's opslaan met audio    3.  stuk uit frame uitklikken gebasseerd  op de huidige video control inputs        3.1  kijken of dat lukt met input van de website en                dat de frames dan nog snel genoeg uitgelezen worden                ik denk het niet maar hoop het wel. anders moeten we"""import cv2import numpy as npimport loggingfrom ffpyplayer.player import MediaPlayerimport time# logging.basicConfig(format='%(asctime)s %(message)s')logging.basicConfig(level=logging.DEBUG)## logger = logging.getLogger()# logger.disabled = Falsevid_right_path = 'assets/videos/right.mp4'vid_left_path = 'assets/videos/left.mp4'frame_count = 0x_shift = 0y_shift = 0zoom = 0resolution_width = 1920resolution_height = 1080wait_for_key_per_frame = Falsemute_audio = Trueright_has_audio = Trueleft_has_audio = Falseplay_left = Trueplay_right = Falsedef time_it(func):    def wrapper(*args, **kwargs):        start_time = time.time()        result = func(*args, **kwargs)        end_time = time.time()        elapsed_time_seconds = end_time - start_time        elapsed_time_ms = elapsed_time_seconds * 1000        logging.debug(f"{func.__name__}"                     f" executed in {elapsed_time_seconds:.4f} seconds"                     f" (in {elapsed_time_ms:.2f} ms)"                     )        return result    return wrapper@time_itdef stitch_images(img_left, img_right):    logging.debug('start stitching images')    # image_right = self.shift_image_vertically(img_left, right_y_shift)    stitched_frame = np.concatenate((img_left, img_right), axis=1)    logging.debug('finished stitching images')    return stitched_frame@time_itdef apply_video_controls(img):    # VID CONTROLS HERE    return img@time_itdef resize_img(img):    # resize based on res width and height    dim = (resolution_width, resolution_height)    # resize image    # TODO: WHEN CUDA WORKS USE THIS:    #   cv2.cuda.resize()    resized = cv2.resize(img, dim, interpolation=cv2.INTER_LINEAR)    return resized@time_itdef show_frame_in_webpage(img):    pass@time_itdef show_frame_opencv(img, audioplayer):    # video    cv2.imshow('final frame', img)    # audio    if audio_player is not None:        audio_frame, ret = audioplayer.get_frame()        if ret != 'eof' and audio_frame is not None:            # audio            img, t = audio_frame@time_itdef read_next_file():    return None@time_itdef stop_video_stream():    logging.info('stopping stream')    # When everything done, release the video capture object    if play_left:        vid_right.release()    if play_right:        vid_left.release()    # Closes all the frames    cv2.destroyAllWindows()# open video's // Download from:# https://h2909571.stratoserver.net/Recordings/2020-03-07-14-29-01/if play_left:    logging.info('opening video file (left stream): ' + vid_left_path)    vid_left = cv2.VideoCapture(vid_left_path)if play_right:    logging.info('opening video file (right stream) ' + vid_right_path)    vid_right = cv2.VideoCapture(vid_right_path)audio_player = Noneif not mute_audio:    if left_has_audio:        logging.info('left video has the audio')        audio_player = MediaPlayer(vid_left_path)    elif right_has_audio:        logging.info('right video has the audio')        audio_player = MediaPlayer(vid_right_path)else:    logging.info('audio muted')if play_left:    fps = int(vid_left.get(cv2.CAP_PROP_FPS))elif play_right:    fps = int(vid_right.get(cv2.CAP_PROP_FPS))logging.info('fps: %s', fps)# Read until video is completedwhile 1:    iteration_start_time = time.time()    # Capture frame-by-frame    if play_left:        ret_left, frame_left = vid_left.read()    if play_right:        ret_right, frame_right = vid_right.read()    final = None    if play_left is True and play_right is True:        logging.debug('start processing left and rigth image')        if not ret_left or not ret_right:            logging.debug('implement searching for next vid here ')            # TODO: IMPLEMENT SEARCHING FOR NEXT VIDEO HERE INSTEAD OF STOPPING AT ONCE            stop_video_stream()        else:            logging.debug('succesfully read frame left and right')            stitched_frame = stitch_images(img_left=frame_left, img_right=frame_right)            # set values shift and zoom using keyboard and mouse            applied_vid_controls_frame = apply_video_controls(stitched_frame)            final = resize_img(applied_vid_controls_frame)            # show frame in webpage? keen ideetjes hoe dat moet            # ik denk dat we elk frame moeten wegschrijven, elke x minuten (5?)            # dan die gewoon uitlezen met js            # show_frame_in_webpage(frame)    elif play_left:        if ret_left:            final = resize_img(frame_left)        else:            stop_video_stream()    elif play_right:        if ret_right:            final = resize_img(frame_right)        else:            stop_video_stream()    # show frame in opencv    show_frame_opencv(final, audio_player)    if wait_for_key_per_frame:        # any key        cv2.waitKey(0)    else:        time.sleep(1 / fps)    iteration_end_time = time.time()    iteration_elapsed_time_in_sec = iteration_end_time - iteration_start_time    iteration_elapsed_time_in_ms = iteration_elapsed_time_in_sec * 1000    logging.info(f"{frame_count}"                 f" executed in {iteration_elapsed_time_in_sec:.4f} seconds"                 f" (in {iteration_elapsed_time_in_ms:.2f} ms)"                 )    frame_count += 1    # Press Q on keyboard to  exit    if cv2.waitKey(25) & 0xFF == ord('q'):        break